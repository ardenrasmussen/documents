\documentclass[12pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\title{Linear Algebra Review}
\author{Arden Rasmussen}
\date{28 February 2017}
\linespread{1}
\begin{document}
\pagenumbering{gobble}
\maketitle
\newpage
\tableofcontents
\newpage
\pagenumbering{arabic}
\section{Determinant}
\subsection{Solving}
\subsubsection{Cofactor Expansion}
\begin{align}
	det(A) &= det\left(\begin{array}{ccc}
	\alpha_{1,1} & \alpha_{1,2} & \alpha_{1,3}\\
	\alpha_{2,1} & \alpha_{2,2} & \alpha_{2,3}\\
	\alpha_{3,1} & \alpha_{3,2} & \alpha_{3,3}
	\end{array}\right)\\
	det(A) &= \alpha_{1,1}[\alpha_{2,2}\alpha_{3,3}] - \alpha_{1,2}[\alpha_{2,1}\alpha_{3,3}] - \alpha_{3,1}\alpha_{2,3}]\\
	&+\alpha_{1,3}[\alpha_{3,2}\alpha_{3,2} - \alpha_{3,1}\alpha_{2,2}]
\end{align}
\subsubsection{Gaussian Operations}
using row and column operations create a row echelon matrix from $A$, by folowing the rules;
\begin{enumerate}
	\item Swapping two rows multiplies the determinant by $-1$.
	\item Multipling a row by a nonzero scalar multiplies the determinant by the same scalar.
	\item Adding to one row a scalar multiple of another does not change the determinant.
\end{enumerate}
\begin{align}
	B &= ref(A)\\
	d &= \text{The product of scalars by which the determinant has been multiplied}\\
	det(A) &= \frac{\prod diagonal(B)}{d}
\end{align}
\subsubsection{Permutations}
$sign(p)$ is the number of swaps necesary to achive that permutation.
\begin{align}
	det(A) = \sum_{p} sign(p)\alpha_{1,p(1)}\alpha_{2,p(2)}\cdots\alpha_{n,p(n)}
\end{align}
\subsection{Properties}
\begin{enumerate}
	\item[$det(A) = 0$] Then $A$ is singular and $A^{-1}$ does not exist.
	\item[$det(A) \neq 0$] Then $A$ is invertible and $A^{-1}$ exists.
\end{enumerate}
\section{Inverse}
\subsection{Gaussian Elimination}
Using row operations only, convert the matrix on the left form $A$ to $I$, and the resulting matrix on the right will be $A^{-1}$.
\begin{align}
	A&\vert I
	I&\vert A^{-1}
\end{align}
\subsection{Cofactor}
\begin{align}
	A^{-1} &= \frac{1}{det(A)} \left[ \begin{array}{ccc}
		\widehat{A_{1,1}} & \widehat{A_{1,2}} & \widehat{A_{1,3}}\\
		\widehat{A_{2,1}} & \widehat{A_{2,2}} & \widehat{A_{2,3}}\\
		\widehat{A_{3,1}} & \widehat{A_{3,2}} & \widehat{A_{3,3}}
	\end{array}\right]^{T}\\
	\widehat{A_{1,1}} &= det \left( \begin{array}{cc}
		\alpha_{2,2} & \alpha_{2,3}\\
		\alpha_{3,2} & \alpha_{3,3}
	\end{array} \right)
\end{align}
\subsection{Properties}
\begin{align}
	(AB)^{-1} &= B^{-1}A^{-1}
	A^{-1}A &= I
	AA^{-1} &= I
\end{align}
\section{Transposse}
Transpose is found by swapping the rows and columns of a matrix.
\begin{align}
	A^{T} = \left[ \begin{array}{ccc}
		\alpha_{1,1} & \alpha_{2,1} & \alpha_{3,1}\\
		\alpha_{1,2} & \alpha_{2,2} & \alpha_{3,2}\\
		\alpha_{1,3} & \alpha_{2,3} & \alpha_{3,3}
	\end{array}\right]
\end{align}
\subsection{symmetric and Anti-Symmetric}
If a matrix is symmetric then:
\begin{align}
	A^{T} = A
\end{align}
If a matrix is anti-symmetric then:
\begin{align}
	A^{T} = -A
\end{align}
\subsection{Properties}
\begin{align}
	(AB)^{T} = B^{T}A^{T}
\end{align}
\section{Rank and Nullity}
Rank is the dimension of the image, nullity is the dimension of the kernel of A. Kernel is the vectors that are collaped to a point.
\begin{align}
	\text{For } &A_{nxn}\\
	Rank(A)&=dim(Im(A))\\
	Nullity(A)&=dim(Ker(A))\\
	det(A)\neq0\Rightarrow \text{Columns are linear}&\text{independent} \Rightarrow Rank(A)=n\\
	Rank(A) + Nullity(A) &=n
\end{align}
\subsection{Solve for Rank}
\begin{align}
	\text{Use Gaussian Elimination on } &A\\
	Rank(A) = \text{The number of pivots}
\end{align}
\subsection{Image and Pre-Image}
\begin{align}
	A\vec{v}&=\vec{w}\\
	\vec{v}&\text{ is the pre-image of } \vec{w}\\
	\vec{w}&\text{ is the image of }\vec{w}
\end{align}
\section{Eigenvalues and Eigenvectors}
\begin{align}
	A\vec{v} = \lambda\vec{v}
\end{align}
\subsection{Eigenvalues}
$\lambda$ is an eigenvalue of $A$ if $A\vec{v} =\lambda\vec{v}$ and $\vec{v}$ is non zero. To find eigen values solve the folowing equation fo $\lambda$.
\begin{align}
	det(A-\lambda I) = 0
\end{align}
\subsection{Eigenvectors}
$\vec{v}$ is an eigenvector of $A$ if $A\vec{v}=\lambda\vec{v}$ and $\vec{v}$ is non zero. To find eigenvectors find the rref of the augmented matrix.
\begin{align}
	\vec{v} = rref(A-\lambda I)
\end{align}
\section{Linear Independance}
Vectors are linear independant if the nullity of the matrix formed by the vectors is zero.
\begin{align}
	\vec{v_{1}}\ &\vec{v_{2}}\ \vec{v_{3}}\\
	Nullity&\left(\begin{array}{ccc}\vec{v_{1}} & \vec{v_{2}} & \vec{v_{3}}\end{array}\right) = 0\\
	&\Rightarrow \text{$\vec{v_{1}}$, $\vec{v_{2}}$, and $\vec{v_{3}}$ are linear independent.}
\end{align}
\section{Basis}
A basis is a span of linear independent vectors which can express everything (within the space) as a linear comination of the vectors.
\section{Matrix Properties}
\subsection{Algebra}
\subsubsection{Addition}
Matrix addition is done term by term.
\begin{align}
	\left[\begin{array}{ccc}
		\alpha_{1,1} & \alpha_{1,2} & \alpha_{1,3}\\
		\alpha_{2,1} & \alpha_{2,2} & \alpha_{2,3}\\
		\alpha_{3,1} & \alpha_{3,2} & \alpha_{3,3}
	\end{array}\right]+\left[\begin{array}{ccc}
		\beta_{1,1} & \beta_{1,2} & \beta_{1,3}\\
		\beta_{2,1} & \beta_{2,2} & \beta_{2,3}\\
		\beta_{3,1} & \beta_{3,2} & \beta_{3,3}
	\end{array}\right]=\left[\begin{array}{ccc}
		\alpha_{1,1} + \beta_{1,1} & \alpha_{1,2} + \beta_{1,2} & \alpha_{1,3} + \beta_{1,3}\\
		\alpha_{2,1} + \beta_{2,1} & \alpha_{2,2} + \beta_{2,2} & \alpha_{2,3} + \beta_{2,3}\\
		\alpha_{3,1} + \beta_{3,1} & \alpha_{3,2} + \beta_{3,2} & \alpha_{3,3} + \beta_{3,3}
	\end{array}\right]
\end{align}
\subsubsection{Subtraction}
Matrix subtraciton is done term by term.
\begin{align}
	\left[\begin{array}{ccc}
		\alpha_{1,1} & \alpha_{1,2} & \alpha_{1,3}\\
		\alpha_{2,1} & \alpha_{2,2} & \alpha_{2,3}\\
		\alpha_{3,1} & \alpha_{3,2} & \alpha_{3,3}
	\end{array}\right]-\left[\begin{array}{ccc}
		\beta_{1,1} & \beta_{1,2} & \beta_{1,3}\\
		\beta_{2,1} & \beta_{2,2} & \beta_{2,3}\\
		\beta_{3,1} & \beta_{3,2} & \beta_{3,3}
	\end{array}\right]=\left[\begin{array}{ccc}
		\alpha_{1,1} - \beta_{1,1} & \alpha_{1,2} - \beta_{1,2} & \alpha_{1,3} - \beta_{1,3}\\
		\alpha_{2,1} - \beta_{2,1} & \alpha_{2,2} - \beta_{2,2} & \alpha_{2,3} - \beta_{2,3}\\
		\alpha_{3,1} - \beta_{3,1} & \alpha_{3,2} - \beta_{3,2} & \alpha_{3,3} - \beta_{3,3}
	\end{array}\right]
\end{align}
\subsubsection{Multiplication}
For matrix multiplicaiton to work, the columns of the first matrix must match the rows of the second matrix.
\begin{align}
	A &\text{ is a nxm matrix}\\
	B &\text{ is a mxp matrix}\\
	A &= \left[\begin{array}{cccc}
		A_{1,1} & A_{1,2} & \cdots & A_{1,m} \\ 
		A_{2,1} & A_{2,2} & \cdots & A_{2,m} \\ 
		\vdots & \vdots & \ddots & \vdots \\ 
		A_{n,1} & A_{n,2} & \cdots & A_{n,m}
	\end{array}\right]\\
	B &= \left[\begin{array}{cccc}
		B_{1,1} & B_{1,2} & \cdots & B_{1,p}\\
		B_{2,1} & B_{2,2} & \cdots & B_{2,p}\\
		\vdots & \vdots & \ddots & \vdots \\
		B_{m,1} & B_{m,2} & \cdots & B_{m,p}
	\end{array}\right]\\
	AB &= \left[\begin{array}{cccc}
		(AB)_{1,1} & (AB)_{1,2} & \cdots & (AB)_{1,p}\\
		(AB)_{2,1} & (AB)_{2,2} & \cdots & (AB)_{2,p}\\
		\vdots & \vdots & \ddots & \vdots \\
		(AB)_{n,1} & (AB)_{n,2} & \cdots & (AB)_{n,p}
	\end{array}\right]\\
	(AB)_{i,j} &= \sum_{k=1} A_{i,k} B_{k,j}
\end{align}
\subsubsection{Division}
Matrix division is just multiplicaiton by the inverse matrix.
\begin{align}
	\frac{B}{A} = B * \frac{1}{A} = B * A^{-1}
\end{align}
\subsection{Commutative and Associative}
\begin{align}
	AB &\neq BA\\
	ABC = (A&B)C = A(BC)
\end{align}
\end{document}
