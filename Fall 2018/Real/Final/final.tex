\documentclass[12pt]{amsart}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{enumitem}
\usepackage{amssymb}
\usepackage[margin=1in]{geometry}
\usepackage{todonotes}

\title{Real Analysis Fall 2018}
\author{Arden Rasmussen}
\date{\today}

\newcommand{\usec}[1]{%
\section*{#1}%
\noindent\rule{\textwidth}{2pt}%
\vspace{10pt}%
}

\definecolor{red}{RGB}{244,67,54}
\definecolor{pink}{RGB}{233,30,99}
\definecolor{purple}{RGB}{156,39,176}
\definecolor{deep-purple}{RGB}{103,58,183}
\definecolor{indigo}{RGB}{117,125,232}
\definecolor{blue}{RGB}{110,198,255}
\definecolor{pale-blue}{RGB}{103,218,255}
\definecolor{cyan}{RGB}{98,239,255}
\definecolor{teal}{RGB}{82,199,184}
\definecolor{green}{RGB}{128,226,126}
\definecolor{pale-green}{RGB}{190,246,122}
\definecolor{lime}{RGB}{255,255,110}
\definecolor{yellow}{RGB}{255,255,114}
\definecolor{amber}{RGB}{255,243,80}
\definecolor{orange}{RGB}{255,201,71}
\definecolor{deep-orange}{RGB}{255,138,80}
\definecolor{brown}{RGB}{169,130,116}

\newcommand{\rtodo}[1]{\todo[inline, color=red]{#1}}
\newcommand{\ytodo}[1]{\todo[inline, color=orange]{#1}}
\newcommand{\btodo}[1]{\todo[inline, color=blue]{#1}}
\newcommand{\gtodo}[1]{\todo[inline, color=green]{#1}}

\newcommand{\prop}[1]{Prop. #1}
\newcommand{\prob}[1]{Prob. #1}
\newcommand{\defn}[1]{Defn. #1}
\newcommand{\thm}[1]{Thm. #1}

\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\F}{\mathbb{F}}


\begin{document}
\maketitle

\usec{Problem 1}%
\label{sec:problem_1}

Show that the exponential function is differentiable, and that $\exp'=\exp$.

\begin{proof}
  Consider the definition of $\exp(x)$ from \defn{4.21}. We consider for some
  $n\in\N$
  \begin{align*}
    E_n(x)&=\sum_{k=0}^n\frac{x^k}{k!}\\
          &=1+x+\frac{x^2}{2!}+\frac{x^3}{3!}+\cdots.
  \end{align*}
  Consider the derivative of $E_n(x)$
  \begin{align*}
    E_n'(x)&=0+1+x+\frac{x^2}{2!}+\cdots\\
           &=\sum_{k=0}^{n-1}\frac{x^k}{k!}.
  \end{align*}
  as $n\to\infty$, from \defn{4.21}, clearly
  \begin{align*}
    E_n(x)&\to\exp(x)\\
    E_n'(x)&\to\exp(x).
  \end{align*}
  Thus by \prop{4.16} we can conclude that $\exp'(x)=\exp(x)$. Thus since the
  derivative exists we conclude that $\exp(x)$ is differentiable.
\end{proof}

\usec{Problem 2}%
\label{sec:problem_2}

Show that $\exp(x)>0$ for all $x\in \R$.

\begin{proof}
  Consider $\exp(x)$, and $x\in\R$.

  \paragraph{Case 1}%
  \label{par:case_1}

  If $x>0$ then
  \begin{align*}
    \exp(x)&=\sum_{k=0}^{\infty}\frac{x^k}{k!}\\
           &=\sum_{k=0}^{\infty}\frac{(+)^k}{(+)}\\
           &=\sum_{k=0}^{\infty}(+)\\
           &> 0
  \end{align*}
  since this is a sum of only positive values, then the sum must also be
  positive. Thus $\exp(x)>0$ if $x>0$.

  \paragraph{Case 2}%
  \label{par:case_2}

  If $x=0$ then
  \begin{align*}
    \exp(0)&=\sum_{k=0}^\infty\frac{0^k}{k!}\\
           &=1+0+0+0+\cdots\\
           &=1\\
           &>0
  \end{align*}
  Since $1$ is greater than $0$ then $\exp(0)>0$.

  \paragraph{Case 3}%
  \label{par:case_3}

  If $x<0$ then we utilize a later proof. We notice that in our proof from
  \prob{4}, we never use the result of this proof. Thus we are able to use the
  result of \prob{4} in this proof. Thus we consider
  \begin{align*}
    \frac{\exp(x)}{\exp(x)} &= 1\\
    \exp(x)\exp(-x)&=1.
  \end{align*}
  Since $-x>0$ and we have shown that $\exp((+))>0$ then we know
  \begin{align*}
    \exp(x)(+)&=(+).
  \end{align*}
  Thus the only way this is true is if $\exp(x)>0$.

  \ytodo{Try to figure out actual method of doing this. Not this cheaty way.}

  We conclude that $\exp(x)>0$ for all $x\in\R$.
\end{proof}

\usec{Problem 3}%
\label{sec:problem_3}

Use the fact that $\exp(x)>0$ for all $x\in\R$ to conclude that the exponential
function is strictly increasing, and thus a bijection $\R\to(0,\infty)$.

\ytodo{Check work for $(-)$ values, as Paul said that was a hard one!}

\begin{proof}
  Consider $\exp(x)$. From \prob{2} $\exp(x)>0$ for all $x\in\R$. Since
  $\exp(x)$ is continuous, from $[-L,L]$ for any $L>0$, we consider some
  sub interval defined by $[\alpha,\beta]$, such that
  $[\alpha,\beta]\subset[-L,L]$. Thus we know that $\exp(x)$ is continuous on
  the interval $[\alpha,\beta]$, and from \prob{1}, it is also differentiable
  on the interval $(\alpha,\beta)$. Thus by the Mean Value Theorem there exists
  some $z\in[\alpha,\beta]$ such that
  \begin{align*}
    \exp'(x)&=\frac{\exp(\beta)-\exp(\alpha)}{\beta-\alpha}.
  \end{align*}
  From \prob{1} we substitute $\exp'(x)=\exp(x)$, then due to the assumption
  that $\exp(x)>0$ for all $x\in\R$, we know that $\exp(z)>0$. From the
  construction of the interval $[\alpha,\beta]$, it is clear that
  \begin{align*}
    \beta&>\alpha\\
    \beta-\alpha&>0.
  \end{align*}
  Thus
  \begin{align*}
    (+)&=\frac{\exp(\beta)-\exp(\alpha)}{(+)}\\
    (+)&=\exp(\beta)-\exp(\alpha)\\
    \implies 0&<\exp(\beta)-\exp(\alpha)\\
    \exp(\alpha)&<\exp(\beta)
  \end{align*}
  Thus we can clearly see that
  \begin{align*}
    \beta>\alpha\implies\exp(\beta)>\exp(\alpha)
  \end{align*}
  Thus we conclude that $\exp(x)$ must be strictly increasing.
\end{proof}

\begin{proof}
  We now prove that $\exp$ is a bijection from $\R\to(0,\infty)$.

  \paragraph{Injective}%
  \label{par:injective}
  First we prove that $\exp$ is injective. This means that
  $\exp(x)=\exp(y)\implies x=y$. Take $x,y\in\R$ and assume that
  $\exp(x)=\exp(y)$. We proceed by contradiction. Assume that $x>y$ (without
  loss of generality, this can be applied to $y>x$). Thus by the previous proof
  \begin{align*}
    x>y\implies \exp(x)>\exp(y).
  \end{align*}
  But this is a contradiction of our assumption that $\exp(x)=\exp(y)$. Using
  the same argument for if $y>x$, we find that it must be true that $x=y$,
  because $x\ngtr y$ and $y\ngtr x$. Thus $\exp$ is injective

  \paragraph{Surjective}%
  \label{par:surjective}
  Now we prove that that for all $y\in(0,\infty)$ there is some $x\in\R$ such
  that $\exp(x)=y$. Take $y\in(0,\infty)$. We construct two sequences of
  numbers defined as
  \begin{align*}
    x_n\to y\quad \alpha_n=\exp(x_n)\\
    z_n\to y\quad \gamma_n=\exp(z_n)
  \end{align*}
  such that $x_n<y<z_n$. By the proof above, we can see that
  \begin{align*}
    \exp(x_n)<&\exp(y)<\exp(z_n)\\
    \alpha_n < &\exp(y) < \beta_n.
  \end{align*}
  Then as $n\to\infty$ by squeeze theorem, we can clearly see that $\alpha_n$
  converges to some value $\gamma$, and $\beta_n$ also converges to this gamma.
  We call this $\gamma=\exp(y)$. Thus for any $y\in(0,\infty)\ \exists
  \gamma\in\R$ such that $\gamma=\exp(y)$. Thus $\exp$ is surjective.

  Since $\exp$ is both injective and surjective, then $\exp$ must be a
  bijection from $\R\to(0,\infty)$.
\end{proof}

\usec{Problem 4}%
\label{sec:problem_4}

In this problem you show that $\exp(a+b)=\exp(a)\exp(b)$ for all $a,b\in\R$. To
do this, fix $a\in \R$ and define the function $g:\R\to\R$ by
\begin{align*}
  g(x)=\exp(a+x)-\exp(a)\exp(x)
\end{align*}
The plan of the proof is to show that $g(x)=0$ for all $x\in\R$.

\subsection*{(a)}%
\label{sub:_a_}

Show that $g$ is differentiable with $g'=g$.

\begin{proof}
  Let $a\in\R$, and consider $g(x)=\exp(a+x)-\exp(a)\exp(x)$, where $x\in\R$.
  Consider the derivative of $g(x)$
  \begin{align*}
    g'(x)&=(\exp(a+x)-\exp(a)\exp(x))'\\
         &\text{by \thm{4.6.a}, and $\exp(a)$ is a constant}\\
         &=(\exp(a+x))'-\exp(a)(\exp(x))'\\
         &\text{by \thm{4.7}}\\
         &=\exp'(a+x)(a+x)'-\exp(a)\exp'(x)\\
         &\text{by \prob{1}}\\
         &=\exp(a+x)-\exp(a)\exp(x)\\
    g'(x)&=g(x)
  \end{align*}
  Thus $g$ is differentiable with $g'=g$.
\end{proof}

\subsection*{(b)}%
\label{sub:_b_}

Show that $g(0)=0$.

\begin{proof}
  Take $g(x)$,
  \begin{align*}
    g(0)&=\exp(a+0)-\exp(a)\exp(0)\\
        &=\exp(a)-\exp(a)\exp(0)\\
        &\text{from \prob{2.2}, $\exp(0)=1$}\\
        &=\exp(a)-\exp(a)\\
        &=0
  \end{align*}
  Thus $g(0)=0$.
\end{proof}

\subsection*{(c)}%
\label{sub:_c_}

Let $M=\sup\left\{|g(x)|:x\in\left[-\frac{1}{4},\frac{1}{4}\right]\right\}$.
Use the Mean Value Theorem to show that $|g(x)|\leq\frac{1}{4}M$ for all
$x\in\left[-\frac{1}{4},\frac{1}{4}\right]$. Deduce that $M\leq\frac{1}{4}M$
and thus $M=0$. Conclude $g(x)=0$ for all
$x\in\left[-\frac{1}{4},\frac{1}{4}\right]$.

\begin{proof}
  We consider the function $g(x)$ as defined above. Let
  \begin{align*}
    M=\sup\left\{|g(x)|:x\in\left[-\frac{1}{4},\frac{1}{4}\right]\right\}.
  \end{align*}

  Since $g$ is continuous because $\exp$ is continuous, and we have shown that
  $g$ is differentiable in \prob{4.a}, we can conclude that $|g|$ will also be
  continuous and differentiable.

  Consider the bounds $\left[0, x\right]$ for $x\in\left[-\frac{1}{4},
  \frac{1}{4}\right]$. Clearly $|g|$ is continuous over the interval $[0,x]$,
  and $|g|$ is differentiable on the interval $(0,x)$. Thus by the Mean Value
  Theorem we know that there exists some $z$ such that
  \begin{align*}
    \left| g'(z)\right|&=\frac{|g(x)|-|g(0)|}{|x-0|}\\
                       &\text{by \prob{4.b}}\\
                       &=\frac{|g(x)|}{|x|}\\
             |x||g'(z)|&=|g(x)|\\
                       &=\text{by \prob{4.a}}\\
              |x||g(z)|&=|g(x)|.
  \end{align*}
  Then by the constraints on $x$ and the assumption of $M$ we find
  \begin{align*}
    \frac{1}{4}M&\geq|x||g(z)|=|g(x)|\\
    \frac{1}{4}M&\geq|g(x)|
  \end{align*}
  This expression holds for all $x\in\left[-\frac{1}{4},\frac{1}{4}\right]$. We
  note that $|g|$ must be bounded, as $\exp$ is bounded on the interval
  $\left[-\frac{1}{4},\frac{1}{4}\right]$, since $|g|$ is bounded, then by the
  definition of supremum of $|g|$ \defn{3.42}, there must exist some value in
  this interval called $x_*$ such that $|g(x_*)|=M$.

  We now use this value in the previous expression
  \begin{align*}
    \frac{1}{4}M&\geq|g(x_*)|\\
    \frac{1}{4}M&\geq M
  \end{align*}
  clearly this can only be the case if $M=0$. By the definition of supremum, it
  is clear that $|g(x)|=0=g(x)$ for all
  $x\in\left[-\frac{1}{4},\frac{1}{4}\right]$.
\end{proof}

\subsection*{(d)}%
\label{sub:_d_}

Adapt the idea of the previous step to show that
\begin{align*}
  g(x_0)=0\implies g(x)=0\ \text{for all}\ x\in\left[x_0-\frac{1}{4},
  x_0+\frac{1}{4}\right].
\end{align*}

\begin{proof}
  We use the same method from the previous proof here. Consider $g(x)$. Assume
  \begin{align*}
    M=\sup\left\{|g(x)|:x\in\left[x_0-\frac{1}{4},x_0+\frac{1}{4}\right]\right\}.
  \end{align*}
  for some $x_0\in\R$ such that $g(x_0)=0$.

  Now consider the interval between $0$ and $x$ for
  $x\in[x_0-\frac{1}{4},x_0+\frac{1}{4}]$. Again by the Mean Value Theorem,
  there exists some $z$ such that
  \begin{align*}
    |g'(z)|&=\frac{|g(x)|-|g(x_0)|}{|x-x_0|}\\
    |g(z)|&=\frac{|g(x)|}{|x-x_0|)}\\
    |x-x_0||g(z)|&=|g(x)|
  \end{align*}
  Because of out limitations on $x$ we know that $|x-x_0|\leq\frac{1}{4}$. And
  by our assumption of $M$ we know
  \begin{align*}
    \frac{1}{4}M\geq|x-x_0||g(z)|&=|g(x)|\\
    \frac{1}{4}M&\geq|g(x)|
  \end{align*}
  By the same reasoning as the previous proof, there must be some $x_*$ in the
  interval such that $|g(x_*)|=M$, thus we can use this value of $x_*$ and find
  \begin{align*}
    \frac{1}{4}M\geq M
  \end{align*}
  Clearly this can only be true if $M=0$, and from this we know that $g(x)=0$
  for all $x\in\left[x_0-\frac{1}{4},x_0+\frac{1}{4}\right]$.
\end{proof}

\subsection*{(e)}%
\label{sub:_e_}

Conclude that $g(x)=0$ for all $x\in\R$.

\begin{proof}
  We begin by using the result of \prob{4.c}, stating that $g(x)=0\ \forall
  x\in\left[-\frac{1}{4},\frac{1}{4}\right]$. We know select $x_1$ to be
  $\frac{1}{4}$. From \prob{4.c} we know that $g(x_1)=0$, and from \prob{4.d},
  we know that this implies that $g(x)=0\ \forall
  x\in\left[x_1-\frac{1}{4},x_1+\frac{1}{4}\right]$. We can repeat this process
  for all increments, by constructing a general expression for $x_n$
  \begin{align*}
    x_n=\frac{n}{4}\quad\forall n\in\Z
  \end{align*}
  Thus for any $x_*\in\R$, there is some interval centered around some $x_n$
  that covers $x_*$ and thus $g(x_*)=0$. Thus we conclude that $g(x)=0$ for all
  $x\in\R$.
\end{proof}

\subsection*{Explain}%
\label{sub:explain}

Explain how this completes the proof that $\exp(a+b)=\exp(a)\exp(b)$ for all
$a,b\in\R$. Show also that $\exp(-a)=1/\exp(a)$ for all $a\in\R$.

Since $g(x)=0\ \forall x\in\R$, then we can see that
\begin{align*}
  0&=\exp(a+x)-\exp(a)\exp(x)\\
  \exp(a+x)&=\exp(a)\exp(x)
\end{align*}
since we made no assumptions on $a$ or $x$ this can be generalized to
\begin{align*}
  \exp(a+b)&=\exp(a)\exp(b)
\end{align*}

Consider
\begin{align*}
  &\exp(0)\\
  &=\exp(-a+a)\\
  &=\exp(-a)\exp(a).
\end{align*}
From \prob{2} we know that $\exp(0)=1$, thus this expression must equal $1$.
\begin{align*}
  1&=\exp(-a)\exp(a)\\
  \frac{1}{\exp(a)}&=\exp(-a)
\end{align*}
Thus $\exp(-a)=1/\exp(a)$.

\usec{Problem 5}%
\label{sec:problem_5}

In this problem you show that $\exp(q)=e^q$ for all $q\in\Q$.

\subsection*{(a)}%
\label{sub:_a_}

Explain why it suffices to show this only for $q>0$.

It is sufficient to show this only for $q>0$, because from \prob{4}, we showed
that $\exp(-a)=\frac{1}{\exp(a)}$, thus is we show this for $q>0$ we know that
\begin{align*}
  \exp(-q)=\frac{1}{\exp{q}}=\frac{1}{e^q}=e^{-q}.
\end{align*}
Thus we are able to neglect the cases when $q<0$, and only prove this for when
$q>0$.

\subsection*{(b)}%
\label{sub:_b_}

Show that $\exp(n)=e^n$ for $n\in\N$.

\begin{proof}
  First we prove that $\exp(1)=e$. From the definition of $\exp$
  \begin{align*}
    \exp(1)&=\sum_{k=0}^\infty\frac{1^k}{k!}\\
           &=\sum_{k=0}^\infty\frac{1}{k!}\\
           &=e
  \end{align*}
\end{proof}

\begin{proof}
   Let $n\in\N$. Consider
   \begin{align*}
     \exp(n)&=\exp(\underbrace{1+1+1+\cdots+1}_\text{$n$ times})\\
            &\text{then by induction and \prob{4}}\\
            &=\underbrace{\exp(1)\exp(1)\cdots\exp(1)}_\text{$n$ times}\\
            &=\exp(1)^n\\
            &=e^n
   \end{align*}
   Thus $\exp(n)=e^n$ for all $n\in\N$.
\end{proof}

\subsection*{(c)}%
\label{sub:_c_}

Show that $\exp(1/m)=e^{1/m}$ for all $m\in\N$.

\rtodo{Figure something out for this! Don't leave it blank.}

\subsection*{(d)}%
\label{sub:_d_}

Conclude that $\exp(q)=e^q$ for $q\in\Q$.

\begin{proof}
   Let $q\in\Q$, such that $q=n/m$ for $n,m\in\N$. Consider
   \begin{align*}
     \exp(q)&=\exp\left(\frac{n}{m}\right)\\
            &=\exp\underbrace{\left(\frac{1}{m}+\frac{1}{m}+\cdots+\frac{1}{m}\right)}_\text{$n$
            times}\\
            &\text{by induction and \prob{4}}\\
            &=\underbrace{\exp\left(\frac{1}{m}\right)\exp\left(\frac{1}{m}\right)\cdots\exp\left(\frac{1}{m}\right)}_\text{$n$
            times}\\
            &=\exp\left(\frac{1}{m}\right)^n\\
            &\text{by \prob{5.c}}\\
            &={e^{1/m}}^n\\
            &=e^{n/m}\\
            &=e^q
   \end{align*}
   Thus $\exp(q)=e^q$ for all $q\in\Q$.
\end{proof}

\usec{Problem 6}%
\label{sec:problem_6}

We define the \textbf{logarithm function} to be the inverse of the exponential
function, so $\log(x)=\exp^{-1}(x)$.

\subsection*{(a)}%
\label{sub:_a_}

Explain how we know that the logarithm function is differentiable and compute
its derivative.

\begin{proof}
  We know that the logarithm is differentiable, because $\exp(x)$ is a
  bijection. And if a function is a bijection, and differentiable, then that
  functions inverse must also be a bijections, and differentiable. Thus $\log$
  must be differentiable.

  We prove by the use of the Inverse Function Theorem (\thm{4.11}). Since
  $\exp$ is differentiable, and since $\exp'=\exp$ clearly, $\exp'$ is
  continuous. From \prob{2}, we know that $\exp'(x_*)\neq 0$ for all $x_*\in\R$,
  since $\exp(x)>0$ for all $x\in\R$. Then, by the Inverse Function Theorem,
  there exists $\delta>0$ such that within $B_\delta(x_*)$ is inevitable, and
  that $\exp^{-1}$ is differentiable and satisfying
  \begin{align*}
    (\exp^{-1})'(y)&=\frac{1}{\exp'(\exp^{-1}(y))}\\
    \log'(y)&=\frac{1}{\exp(\log(y))}.
  \end{align*}
  By the definition of an inverse function, we know that $\exp(\log(x))=x$,
  thus
  \begin{align*}
    \log'(y)&=\frac{1}{y}.
  \end{align*}
  Since we made no assumptions on $x_*$ or $y$, then this is satisfied for all
  $y\in(0,\infty)$.
\end{proof}

\subsection*{(b)}%
\label{sub:_b_}

Show that the ``usual log rules'' hold:
\begin{align*}
  \log(xy)=\log(x)+\log(y),\quad
  \log(x/y)=\log(x)-\log(y),\quad
  \log(x^y)=y\log(x).
\end{align*}

\subsubsection*{$\log(xy)=\log(x)+\log(y)$:}%
\label{ssub:_log_xy_log_x_log_y_}

\begin{proof}
  Consider $\log(xy)$ for any $x,y\in(0,\infty)$. Define some $\alpha$ and
  $\beta$ such that
  \begin{align*}
    \log(x)&=\alpha,\quad&\log(y)&=\beta\\
    \exp(\alpha)&=x,\quad&\exp(\beta)&=y.
  \end{align*}
  Thus we rewrite the expression as
  \begin{align*}
    \log(xy)&=\log(\exp(\alpha)\exp(\beta))\\
            &\text{by \prob{4}}\\
            &=\log(\exp(\alpha+\beta))\\
            &\text{by inverse}\\
            &=\alpha+\beta\\
            &=\log(x)+\log(y)
  \end{align*}
  Thus $\log(xy)=\log(x)+\log(y)$ is true.
\end{proof}

\subsubsection*{$\log(x/y)=\log(x)-\log(y)$:}%
\label{ssub:_log_x_y_log_x_log_y_}

\begin{proof}
  Consider $\log\left(\frac{x}{y}\right)$ for any $x,y\in(0,\infty)$. We again
  define $\alpha,\beta\in\R$, in the same way as before. Thus our expression
  becomes
  \begin{align*}
    \log\left(\frac{x}{y}\right)&=\log\left(\frac{\exp(\alpha)}{\exp(\beta)}\right)\\
                                &\text{by \prob{4}}\\
                                &=\log(\exp(\alpha)\exp(-\beta))\\
                                &=\log(\exp(\alpha-\beta))\\
                                &=\alpha-\beta\\
                                &=\log(x)-\log(y).
  \end{align*}
  Thus $\log(x/y)=\log(x)-\log(y)$ holds.
\end{proof}

\subsubsection*{$\log(x^y)=y\log(x)$:}%
\label{ssub:_log_x_y_ylog_x_}

\begin{proof}
  Consider $\log(x^y)$ for any $x,y\in(0,\infty)$. We again define $\alpha$ in
  the same was as before. Thus we can write
  \begin{align*}
    \log(x^y)&=\log(\exp(\alpha)^y)\\
             &\text{by \prob{5}}\\
             &=\log(e^{\alpha^y})\\
             &=\log(e^{y\alpha})\\
             &=y\alpha\\
             &=y\log(x)
  \end{align*}
  Thus $\log(x^y)=y\log(x)$ holds.
\end{proof}

\usec{Problem 7}%
\label{sec:problem_7}

The real numbers

\subsection*{(a)}%
\label{sub:_a_}

What are the axioms/properties that we are assuming about the real number. What
purpose do each of these axioms serve? (That is, what motivates each of these
axioms?)

The axioms of the real numbers are:
\begin{itemize}
  \item Field axioms
    \begin{itemize}
      \item Addition axioms
        \begin{itemize}
          \item Commutative $x+y=y+x\ \forall x,y\in\F$.
          \item Associative $(x+y)+z=x+(y+z)\ \forall x,y\in\F$.
          \item There exists an Additive Identity $0\in\F$ such that
            $0+x=x\ \forall x\in\F$.
          \item For all $x\in\F$ there exists an additive inverse $-x\in\F$
            such that $x+(-x)=0$.
        \end{itemize}
      \item Multiplication axioms
        \begin{itemize}
          \item Commutative $x\cdot y=y\cdot x\ \forall x,y\in\F$.
          \item Associative $(x\cdot y)\cdot z = x\cdot(y\cdot z)\ \forall
            x,y,z\in\F$.
          \item There exists a multiplicative identity $1\in\F\setminus{0}$
            such that $1\cdot x=x\ \forall x\in\F$.
          \item For all $x\in\F\setminus\{0\}$ there exists a multiplicative
            inverse $x^{-1}\in\F$ such that $x\cdot x^{-1}=1$.
        \end{itemize}
      \item Distributive law $x\cdot(y+z)=x\cdot y+x\cdot z\ \forall
        x,y,z\in\F$.
    \end{itemize}
  \item Ordered fields
    \begin{itemize}
      \item For each $x,y\in\F$ precisely one of the following holds: $x<y$,
        $x=y$, or $y<x$.
      \item if $x,y,z\in\F$ with $x<y$ and $y<z$, then $x<z$.
      \item if $x,y,z\in\F$ with $x<y$ then $x+z<y+z$.
      \item if $x,y\in\F$ with $0<x$ and $0<y$ then $0<x\cdot y$.
    \end{itemize}
  \item Archimedian property. For each $x\in\F$ with $x>0$ there exists
    $n\in\N$ with $n>x$.
  \item Bisection Search is successful. For every sequence of closed intervals
    $I_n$ with
    \begin{enumerate}
      \item $I_n\subset I_{n+1}\ \forall n\in\N$.
      \item $|I_n|\to0$.
    \end{enumerate}
    we have $\bigcap_{n\in\N}I_n\neq\emptyset$.
\end{itemize}

The field axioms provide us with a system where our expressions for addition,
subtraction, multiplication and division work as we desire them to. The ordered
field axiom allows us to construct sequences of real numbers. Then the
archimedian property says that there is some number that is greater than a
given $x$, e.g. there is always a number larger. Bisection search says that
there will always be some value between two other numbers, as the interval goes
to zero, but will always contain some elements.

\gtodo{Write more reasoning for the different axioms.}

\subsection*{(b)}%
\label{sub:_b_}

Define what it means for a sequence $\left\{x_n\right\}\subset\R$ to converge to
$x_*\in\R$.

A sequence $\left\{x_n\right\}\subset \R$ converges to $x_*\in\R$ if for all
$\varepsilon\in\R$ with $\varepsilon>0$ there exists some $N\in\N$ such that
\begin{align*}
   k>N\implies |x_k-x_*|<\varepsilon.
\end{align*}
In this case we write $x_n\to x_*$.

\subsection*{(c)}%
\label{sub:_c_}

Give an example of a theorem (or other fun fact) that makes use of sequences.
What makes this result noteworthy?

A theorem that makes uses of sequences is Squeeze theory (\prop{1.17}). This
theorem is very noteworthy, because it is necessary for other axioms, such at
bisection search. It allows us to say that two things converging toward one
another, will squeeze some third sequence and they will all converge to the
same value. This is incredibly useful and very intuitive.

\gtodo{More?}

\usec{Problem 8}%
\label{sec:problem_8}

Metric spaces

\subsection*{(a)}%
\label{sub:_a_}

Give the definition of a metric space.

Let $X$ be a set. A function $d:X\times X\to[0,\infty)$ is a metric on $X$ if
$d$ is
\begin{enumerate}
  \item definite, meaning that for all $x,y\in X$ we have $d(x,y)=0$ if and
    only if $x=y$.
  \item symmetric meaning that $d(x,y)=d(y,x)$ for all $x,y\in X$, and
  \item satisfies the triangle inequality
    \begin{align*}
      d(x,y)\leq d(x,z)+d(z,y)
    \end{align*}
    for all $x,y,z\in X$.
\end{enumerate}
if $d$ is a metric on $X$ then the pair $(X,d)$ is called a metric space.

\subsection*{(b)}%
\label{sub:_b_}

Give the definition of what it means for a set to be open in a metric space.

Let $Y\subset X$.
\begin{enumerate}
  \item An open ball is defined as $B_r(p)=\left\{q\in X\vert
    d(p,q)<r\right\}$.
  \item A point $p\in Y$ is an interior point if there exists an open ball
    $B_r(q)$ such that $p\in B_r(q)\subset Y$.
  \item $Y$ is open in $X$ if each element of $Y$ is an interior point.
\end{enumerate}

\subsection*{(c)}%
\label{sub:_c_}

Give three examples of metric spaces -- one example where the elements are
number, one example where the elements are pairs of numbers, and one example
where the elements are functions. Give an example of an open ball in each of
your three examples.

\ytodo{Ask Paul if this is what he meant by an example of an open ball, or
should I leave it more general?. Drawing?}

\begin{enumerate}
  \item $\left(\R, |x-y|\right)$.
    \begin{align*}
      B_{0.5}(0)=\left\{y\in\R\ \bigg\vert\ |x-y|<0.5\right\}=(-0.5,0.5).
    \end{align*}
  \item $\left(\R^2,\sqrt{{|x_1-y_1|}^2+{|x_2-y_2|}^2}\right)$.
    \begin{align*}
      B_{0.5}(0)=\left\{y\in\R^2\ \bigg\vert\
      \sqrt{{|x_1-y_1|}^2+{|x_2-y_2|}^2}<0.5\right\}=\left\{y\in\R\ \bigg\vert\
    \sqrt{y_1^2+y_2^2}<0.5\right\}
    \end{align*}
  \item $\left(C_b(X),0 \text{ if } f=g \text{ else } 1\right)\equiv$ All continuous bounded
    functions $f:X\to\R$, with the norm $\Vert f\Vert=\sup_X|f|$ is finite.
    With the discrete metric.
    \begin{align*}
      B_{0.5}(f)=\left\{g\in C_b(X)\ \bigg\vert\ 0\text{ if
      }f=g\right\}=\left\{f\right\}
    \end{align*}
\end{enumerate}

\subsection*{(d)}%
\label{sub:_d_}

Give an example of a theorem (or other fun fact) that makes use of open sets.
What makes this result noteworthy?

An interesting theorem is the open set definition of continuous functions
(\thm{3.23}). This theorem states that a function is continuous if and only if
every open set in the co-domain the preimage of that set is also open in the
domain.

This is a very noteworthy result, as we con construct a notion of continuous
functions without every really having a function, we just need to know how open
sets relate over some mapping. Because we only need to think of the open sets
of a metric space, it is very cool that we can then uses these open sets to
determine weather a function is continuous or not.

\gtodo{More?}

\usec{Problem 9}%
\label{sec:problem_9}

Continuous functions

\subsection*{(a)}%
\label{sub:_a_}

State the four equivalent notions of continuity for functions. Illustrate each
notion for the function $f:\R^2\to\R$ given by $f(x,y)=x^2+y^2$.

Suppose $(X,dx)$ and $(Y,dy)$ are metric spaces and $f:X\to Y$. Then the
following are equivalent.

\begin{enumerate}
  \item $f$ is continuous.
  \item For each $x_*\in X$ and $\varepsilon >0$ there exists $\delta >0$ such
    that
    \begin{align*}
      dx(x,x_*)<\delta\implies dy(f(x),f(x_*))<\varepsilon.
    \end{align*}
  \item For each open set $U\subset Y$ the preimage $f^{-1}(U)$ is open in $X$.
  \item For each closed set $V\subset Y$ the preimage $f^{-1}(V)$ is closed in
    $X$.
\end{enumerate}

% \vspace{4in}
\noindent\fbox{\parbox{\textwidth}{$\ $\vspace{4in}}}

\subsection*{(b)}%
\label{sub:_b_}

Give an example of a theorem (or other fun fact) that makes use of continuous
functions. What makes this result noteworthy?

The basic properties of continuous functions are very fun (\prob{1} Exam 1). I
think that these properties are very cool, because they prove that
combinations of continuous function are then continuous. Because of this all
that needs to be shown is that a very limited number of functions must be
proven to be continuous, then by these properties, combinations, products,
compositions and any combination of these is then also continuous.

This can be especially useful, when showing a function is continuous, if all of
the individual components are continuous, then the function as a whole is also
continuous. This greatly simplifies continuity proofs, and that is very
convenient and useful.

\gtodo{More?}

\usec{Problem 10}%
\label{sec:problem_10}

Synthesis

Write a few words indicating how the ideas about numbers, metric spaces, and
continuous functions discussed in this course help lay the foundation for a
more rigorous theory of calculus.

Using these concepts for numbers, metrics, and functions, we are able to
construct most aspects of the theory of calculus, while only taking the axioms
that we listed in \prob{9.a}. Because these are the only assumptions that need
to be made, the definitions that we construct for calculus are more rigorous.
As they do not require any more assumptions. Normally we calculus requires many
more assumptions in order to construct some of the concepts, but with this
process, we are able to minimize the number of axioms required for the theory
of calculus. This allows for larger generalizability, as anything that
satisfies the short list of assumptions can be applied to use in the theory of
calculus. Thus this method lays the foundation for a more rigorous theory of
calculus.

\gtodo{Write More?}

\end{document}
