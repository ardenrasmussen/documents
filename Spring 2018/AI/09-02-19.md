# Notes

## k-armed bandit

exploration vs explitation

Upper confidence bounds

$$
\\frac{wins}{pulls} + \\frac{expore}{pulls}
$$

With few pulls more exploration.

## Monte Carlo

-   Create a root node
-   Thousands of times:
    -   Play a simulated game from the current state
    -   Update search tree
-   Play best move from root

Each node knows, for each chilk

-   # polls
-   # Wins
